{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  clown  bequeath  juturna  'surpassing  overclean  novo  1st.  \\\n",
      "0           0      0         0        0            0          0     0     0   \n",
      "1           1      0         1        0            0          0     0     0   \n",
      "2           2      1         0        0            0          0     0     0   \n",
      "3           3      1         0        0            0          0     0     0   \n",
      "4           4      0         0        0            0          0     0     0   \n",
      "\n",
      "   side-flirtation  saxons  ...   13.1  14.1  15.1  16.1  17.1  18.1  19.1  \\\n",
      "0                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1                0       0  ...    0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   20.1  21.1  YEAR  \n",
      "0   0.0   0.0  1882  \n",
      "1   0.0   0.0  1863  \n",
      "2   0.0   0.0  1910  \n",
      "3   0.0   1.0  1856  \n",
      "4   0.0   1.0  1864  \n",
      "\n",
      "[5 rows x 191830 columns]\n",
      "(414, 191830)\n",
      "   Unnamed: 0  clown  bequeath  juturna  'surpassing  overclean  novo  1st.  \\\n",
      "0           0      1         1        0            0          0     0     0   \n",
      "1           1      0         1        0            0          0     0     0   \n",
      "2           2      0         0        0            0          0     0     0   \n",
      "3           3      0         0        0            0          0     0     0   \n",
      "4           4      0         0        0            0          0     0     0   \n",
      "\n",
      "   side-flirtation  saxons  ...   13.1  14.1  15.1  16.1  17.1  18.1  19.1  \\\n",
      "0                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   20.1  21.1  YEAR  \n",
      "0   0.0   0.0  1894  \n",
      "1   1.0   0.0  1901  \n",
      "2   0.0   0.0  1891  \n",
      "3   0.0   0.0  1866  \n",
      "4   0.0   0.0  1885  \n",
      "\n",
      "[5 rows x 191830 columns]\n",
      "(138, 191830)\n",
      "   Unnamed: 0  clown  bequeath  juturna  'surpassing  overclean  novo  1st.  \\\n",
      "0           0      0         0        0            0          0     0     0   \n",
      "1           1      0         1        0            0          0     0     0   \n",
      "2           2      1         0        0            0          0     0     0   \n",
      "3           3      1         0        0            0          0     0     0   \n",
      "4           4      0         0        0            0          0     0     0   \n",
      "\n",
      "   side-flirtation  saxons  ...   13.1  14.1  15.1  16.1  17.1  18.1  19.1  \\\n",
      "0                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1                0       0  ...    0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4                0       0  ...    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   20.1  21.1  YEAR  \n",
      "0   0.0   0.0  1882  \n",
      "1   0.0   0.0  1863  \n",
      "2   0.0   0.0  1910  \n",
      "3   0.0   1.0  1856  \n",
      "4   0.0   1.0  1864  \n",
      "\n",
      "[5 rows x 191830 columns]\n",
      "(552, 191830)\n",
      "finished...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#将之前错误分开的train和test先合并在一起\n",
    "def join_set(set1,set2,set3):\n",
    "    df1 = pd.read_csv(set1)\n",
    "    print(df1.head())\n",
    "    print(df1.shape)\n",
    "    df2 = pd.read_csv(set2)\n",
    "    print(df2.head())\n",
    "    print(df2.shape)\n",
    "    df = df1.append(df2)\n",
    "    print(df.head())\n",
    "    print(df.shape)\n",
    "    df.to_csv(set3)\n",
    "    print(\"finished...\")\n",
    "    \n",
    "join_set(\"DCLSA/la_dataset_split/all_lemma_train_matrix.csv\",\"DCLSA/la_dataset_split/all_lemma_test_matrix.csv\",\n",
    "         \"DCLSA/la_dataset_split/all_lemma_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  (  EX  CD  FW  NNS  NNP  POS  WP  VBD  ...    13   14   15  \\\n",
      "0           0  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "1           1  1   1   1   1    1    1    1   1    1  ...   0.0  1.0  0.0   \n",
      "2           2  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "3           3  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "4           4  0   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "\n",
      "    16   17   18   19   20   21  YEAR  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  1882  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  1863  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  1910  \n",
      "3  0.0  0.0  0.0  0.0  0.0  1.0  1856  \n",
      "4  0.0  0.0  0.0  0.0  0.0  1.0  1864  \n",
      "\n",
      "[5 rows x 68 columns]\n",
      "(414, 68)\n",
      "   Unnamed: 0  (  EX  CD  FW  NNS  NNP  POS  WP  VBD  ...    13   14   15  \\\n",
      "0           0  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "1           1  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "2           2  0   1   1   1    1    1    0   1    1  ...   0.0  0.0  0.0   \n",
      "3           3  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "4           4  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "\n",
      "    16   17   18   19   20   21  YEAR  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  1894  \n",
      "1  0.0  0.0  0.0  0.0  1.0  0.0  1901  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  1891  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  1866  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  1885  \n",
      "\n",
      "[5 rows x 68 columns]\n",
      "(138, 68)\n",
      "   Unnamed: 0  (  EX  CD  FW  NNS  NNP  POS  WP  VBD  ...    13   14   15  \\\n",
      "0           0  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "1           1  1   1   1   1    1    1    1   1    1  ...   0.0  1.0  0.0   \n",
      "2           2  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "3           3  1   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "4           4  0   1   1   1    1    1    1   1    1  ...   0.0  0.0  0.0   \n",
      "\n",
      "    16   17   18   19   20   21  YEAR  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  1882  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  1863  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  1910  \n",
      "3  0.0  0.0  0.0  0.0  0.0  1.0  1856  \n",
      "4  0.0  0.0  0.0  0.0  0.0  1.0  1864  \n",
      "\n",
      "[5 rows x 68 columns]\n",
      "(552, 68)\n",
      "finished...\n"
     ]
    }
   ],
   "source": [
    "join_set(\"DCLSA/la_dataset_split/all_pos_train_matrix.csv\",\"DCLSA/la_dataset_split/all_pos_test_matrix.csv\",\n",
    "         \"DCLSA/la_dataset_split/all_pos_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  unhandicapped.VBD  threequarters.NNS  hay-riggings.NNS  \\\n",
      "0           0                  0                  0                 0   \n",
      "1           1                  0                  0                 0   \n",
      "2           2                  0                  0                 0   \n",
      "3           3                  0                  0                 0   \n",
      "4           4                  0                  0                 0   \n",
      "\n",
      "   instruct.VBD  sealegs.JJ  parent-honoring.NN  Riparia.NNP  dntnken.NN  \\\n",
      "0             0           0                   0            0           0   \n",
      "1             1           0                   0            0           0   \n",
      "2             0           0                   0            0           0   \n",
      "3             1           0                   0            0           0   \n",
      "4             0           0                   0            0           0   \n",
      "\n",
      "   germain.NN  ...    13   14   15   16   17   18   19   20   21  YEAR  \n",
      "0           0  ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1882  \n",
      "1           0  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1863  \n",
      "2           0  ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1910  \n",
      "3           0  ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1856  \n",
      "4           0  ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1864  \n",
      "\n",
      "[5 rows x 321749 columns]\n",
      "(414, 321749)\n"
     ]
    }
   ],
   "source": [
    "join_set(\"DCLSA/la_dataset_split/all_lexical_train_matrix.csv\",\"DCLSA/la_dataset_split/all_lexical_test_matrix.csv\",\n",
    "         \"DCLSA/la_dataset_split/all_lexical_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22个作家数据的矩阵化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'EX', 'CD', 'FW', 'NNS', 'NNP', 'POS', 'WP', 'VBD', 'RB', '#', 'RP', 'WDT', 'JJR', 'TO', 'JJ', 'CC', 'VB', 'MD', 'DT', 'VBG', '.', 'IN', \"''\", 'RBS', 'WRB', 'VBP', 'VBZ', 'VBN', 'NN', 'UH', 'PDT', 'RBR', 'PRP', 'JJS', ',', 'WP$', ':', 'NNPS', 'SYM', '$', '``', 'PRP$', ')']\n",
      "191806\n",
      "44\n",
      "321725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\qxy09\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#生成22个作家对应all集的matrix，用作22个test集\n",
    "#读取数据\n",
    "def get_data(file):\n",
    "    data = pd.read_csv(\"DCLSA/literature_author_dataset/\"+file)\n",
    "    data.drop(['Unnamed: 0','book'], axis=1,inplace=True)\n",
    "\n",
    "    x,y = data.ix[:,1],data.ix[:,0]\n",
    "    x = x.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "# #获取ab.csv这个数据集的训练集和测试机\n",
    "# ab_x,ab_y = get_data(\"ab.csv\")\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# f = open(\"t.txt\",'rb')\n",
    "# text =  f.read()\n",
    "# text = bytes.decode(text)\n",
    "# f.close()\n",
    "#text = \"ALONG stretch of level country, bordered on one side by the sea, sweeps away until the imagination begins its guesswork where the eye is forced to stop, at a thick fringe of pines in the far background. Here and there, perhaps two miles from the sea, are farm houses in picturesque groups of threes and fours, forming the little town of StratfordbytheSea. This quaint corner of the old township dates back to colonial days, and almost to colonial primitiveness of life, though, like many another place and people, it has retained certain forms and customs of its ancestors without their accompanying rigidity of virtue. But, alas ! true as this is in the main, many a good custom is unhappily obsolete ; there are scolds in Stratford who have made no acquaintance with the ducking stool, and an occasional worthy member of society dares absent himself from church without fear of a penalty.\"\n",
    "\n",
    "#去除停用词\n",
    "\n",
    "#分词\n",
    "def word_feature(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    word_list = list(set(word_list))\n",
    "    return word_list\n",
    "\n",
    "#print(word_feature(text))\n",
    "\n",
    "\"\"\"\n",
    "#feature1：生成stem\n",
    "def stem_feature(text):\n",
    "    word_list = word_feature(text)\n",
    "    stem_list = []\n",
    "    p = PorterStemmer()\n",
    "    for word in word_list:\n",
    "        stem = p.stem(word)\n",
    "        stem_list.append(stem)\n",
    "\n",
    "    stem_list = list(set(stem_list))    \n",
    "    return stem_list\n",
    "\n",
    "print(stem_feature(text))\n",
    "\"\"\"\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "#feature1:生成lemma\n",
    "def lemma_feature(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_list = word_feature(text)\n",
    "    lemma_list = []\n",
    "    for word in word_list:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "        lemma_list.append(lemma)\n",
    "\n",
    "    lemma_list1 = list(set(lemma_list))\n",
    "    \n",
    "    lemma_list = [x.lower() for x in lemma_list1 if isinstance(x,str)] \n",
    "    return lemma_list\n",
    "\n",
    "\n",
    "\n",
    "#feature2：生成pos\n",
    "def pos_feature(text):\n",
    "    word_list = word_feature(text)\n",
    "    pos_list = []\n",
    "    lexical_list = nltk.pos_tag(word_list)\n",
    "    for pos in lexical_list:\n",
    "        pos_list.append(pos[1])    \n",
    "    \n",
    "    pos_list = list(set(pos_list))\n",
    "    return pos_list\n",
    "\n",
    "\n",
    "\n",
    "#feature3:生成lexical（lemma+pos）\n",
    "def lexical_feature(text):\n",
    "    lemma_list = word_feature(text)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lexical_list = nltk.pos_tag(lemma_list)\n",
    "    lexical_list1 = []\n",
    "    for i in lexical_list:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(i[0], get_wordnet_pos(i[0]))\n",
    "        if isinstance(lemma,str):\n",
    "             lemma.lower()\n",
    "        temp =lemma+'.'+i[1]\n",
    "        lexical_list1.append(temp)\n",
    "    \n",
    "    lexical_list1 = list(set(lexical_list1))\n",
    "    return lexical_list1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##提取train 集特征值\n",
    "#1.lemma特征值\n",
    "def lemma_f(train_x):\n",
    "    lemma_f = []\n",
    "    i = 0\n",
    "    for text in train_x:\n",
    "        i = i+1\n",
    "        temp_lemma_list = lemma_feature(text)\n",
    "        lemma_f = list(set(lemma_f).union(set(temp_lemma_list)))\n",
    "        if i%50==0:\n",
    "            print(\"lemma_finished:\"+str(i)+','+str(i/len(train_x))+\"....\")\n",
    "#     print(lemma_f)\n",
    "#     print(len(lemma_f))\n",
    "    return lemma_f\n",
    "  \n",
    "    \n",
    "#2.pos特征值\n",
    "def pos_f(train_x):\n",
    "    pos_f = []\n",
    "    i = 0\n",
    "    for text in train_x:\n",
    "        i = i+1\n",
    "        #print(i)\n",
    "        temp_pos_list = pos_feature(text)\n",
    "        pos_f = list(set(pos_f).union(set(temp_pos_list)))\n",
    "        if i%50==0:\n",
    "            print(\"pos_finished:\"+str(i)+','+str(i/len(train_x))+\"....\")\n",
    "    return pos_f\n",
    "\n",
    "\n",
    "#3.lexical特征值\n",
    "def lexical_f(train_x):\n",
    "    lexical_f = []\n",
    "    i = 0\n",
    "    for text in train_x:\n",
    "        i = i+1\n",
    "        temp_lexical_list = lexical_feature(text)\n",
    "        lexical_f = list(set(lexical_f).union(set(temp_lexical_list)))\n",
    "        if i%50==0:\n",
    "            print(\"lexical_finished:\"++str(i)+','+str(i/len(train_x))+\"....\")\n",
    "    return lexical_f\n",
    "\n",
    "\n",
    "#读取特征\n",
    "#提取all集的特征值\n",
    "with open('all_feature.json', encoding='utf-8') as data_file:\n",
    "    features = json.loads(data_file.read())\n",
    "\n",
    "print(features['pos'])\n",
    "all_lemma_f = features['lemma']\n",
    "all_pos_f = features['pos']\n",
    "all_lexical_f = features['lexical']\n",
    "print(len(all_lemma_f))\n",
    "print(len(all_pos_f))\n",
    "print(len(all_lexical_f))\n",
    "\n",
    "\n",
    "#生成matrix\n",
    "def vectorize(sent,feature_list):\n",
    "    vector = [sent.count(f) for f in feature_list]\n",
    "    return(vector)\n",
    "\n",
    "#train和test set 的文本向量化\n",
    "def create_matrix(x,y,feature_fun,feature_type):\n",
    "#     \"Creates the apporiate feature matrix\"\n",
    "#     mat = pd.read_csv(\"{}.csv\".format(name))\n",
    "#     mat.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "    \n",
    "    origin_list = x\n",
    "    feature_list = list(map(feature_fun,origin_list))\n",
    "    \n",
    "    vectors = [vectorize(s,feature_type) for s in feature_list]\n",
    "    df = pd.DataFrame(vectors, columns=feature_type)\n",
    "    \n",
    "    df['YEAR'] = y\n",
    "    print(y)\n",
    "    print(df['YEAR'])\n",
    "    #df.to_csv(\"test_matrix.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# create_matrix(ab_x,ab_y,lemma_feature,all_lemma_f).to_csv(\"DCLSA/la_dataset_split/ab_lemma_matrix.csv\")\n",
    "# create_matrix(ab_x,ab_y,pos_feature,all_pos_f).to_csv(\"DCLSA/la_dataset_split/ab_pos_matrix.csv\")\n",
    "# create_matrix(ab_x,ab_y,lexical_feature,all_lexical_f).to_csv(\"DCLSA/la_dataset_split/ab_lexical_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dc0874055137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcreate_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlemma_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_lexical_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DCLSA/la_dataset_split/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_lemma_matrix.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mcreate_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_lexical_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DCLSA/la_dataset_split/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_pos_matrix.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcreate_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlexical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_lexical_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DCLSA/la_dataset_split/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_lexical_matrix.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-54a5b01414c9>\u001b[0m in \u001b[0;36mcreate_matrix\u001b[1;34m(x, y, feature_fun, feature_type)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0morigin_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[0mfeature_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_fun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morigin_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"DCLSA/literature_author_dataset\"):\n",
    "    x,y = get_data(filename)\n",
    "    list1 = filename.split('.')\n",
    "    print(list1)\n",
    "    author = list1[0]\n",
    "    create_matrix(x,y,lemma_feature,all_lexical_f).to_csv(\"DCLSA/la_dataset_split/\"+author+\"_lemma_matrix.csv\")\n",
    "    create_matrix(x,y,pos_feature,all_lexical_f).to_csv(\"DCLSA/la_dataset_split/\"+author+\"_pos_matrix.csv\")\n",
    "    create_matrix(x,y,lexical_feature,all_lexical_f).to_csv(\"DCLSA/la_dataset_split/\"+author+\"_lexical_matrix.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-131981e8e4fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DCLSA/la_dataset_split/all_lexical_train_matrix.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# df2=pd.read_csv('DCLSA/la_dataset_split/all_lexical_test_matrix.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(len(df2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m     \"\"\"\n\u001b[0;32m    813\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv('DCLSA/la_dataset_split/all_lexical_train_matrix.csv')\n",
    "print(len(df1))\n",
    "# df2=pd.read_csv('DCLSA/la_dataset_split/all_lexical_test_matrix.csv')\n",
    "# print(len(df2))\n",
    "# df=pd.read_csv('DCLSA/la_dataset_split/all_lexical_matrix.csv')\n",
    "# print(len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
