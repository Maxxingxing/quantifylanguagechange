{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#分词--后可以按频率排布nltk，特征（word或stem的话）可能有无数多个选取前多少还是全部？？，pos相对较少，（四种feature），\n",
    "对应每条entry生成一个矩阵行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.分割train，validation，test集--sklearn\n",
    "  读取tran集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060]\n",
      "[nltk_data]     由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>\n",
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060]\n",
      "[nltk_data]     由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\qxy09\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Louise Imogen Guiney was born in Boston on Jan...\n",
      "1    And now, with the youth of the year, hath come...\n",
      "2    There could not have been a more sympathetic m...\n",
      "3    We who are Tiverton born, though false ambitio...\n",
      "4    THE old Winterbourne house, one of New England...\n",
      "5    When this story begins, the youngest of those ...\n",
      "6    The life of Francis Hume began in an old yet v...\n",
      "7    ALONG stretch of level country, bordered on on...\n",
      "8    Madam Fulton and her granddaughter Electra wer...\n",
      "Name: text, dtype: object\n",
      "0    1921\n",
      "1    1896\n",
      "2    1916\n",
      "3    1895\n",
      "4    1910\n",
      "5    1917\n",
      "6    1897\n",
      "7    1884\n",
      "8    1907\n",
      "Name: year, dtype: int64\n",
      "0    Amelia Maxwell sat by the front-chamber window...\n",
      "1    John Raven sat in the library of his shabby, y...\n",
      "2    Tiverton has breezy, upland roads, and damp, s...\n",
      "Name: text, dtype: object\n",
      "0    1910\n",
      "1    1922\n",
      "2    1899\n",
      "Name: year, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv(\"train.csv\")\n",
    "# train.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "# print(len(train))\n",
    "# train.head()\n",
    "\n",
    "def get_data(file):\n",
    "    data = pd.read_csv(\"DCLSA/literature_author_dataset/\"+file)\n",
    "    data.drop(['Unnamed: 0','book'], axis=1,inplace=True)\n",
    "    #print(len(data))\n",
    "    #data.head()\n",
    "    # print(type(train['year'].tolist()))\n",
    "    # print(train['text'])\n",
    "    #print(train.ix[[0],['text']])\n",
    "\n",
    "    x,y = data.ix[:,1],data.ix[:,0]\n",
    "    #测试集为25%，训练集为75%\n",
    "    train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.25,random_state=0)\n",
    "    # print(len(train_x))\n",
    "    # print(len(train_y))\n",
    "    #print(type(train_x))\n",
    "    # print(len(test_x))\n",
    "    train_x = train_x.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    print(train_x)\n",
    "    print(train_y)\n",
    "    test_x = test_x.reset_index(drop=True)\n",
    "    test_y = test_y.reset_index(drop=True)\n",
    "    print(test_x)\n",
    "    print(test_y)\n",
    "    return train_x,train_y,test_x,test_y\n",
    "\n",
    "#获取ab.csv这个数据集的训练集和测试机\n",
    "ab_train_x,ab_train_y,ab_test_x,ab_test_y = get_data(\"ab.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n",
      "414\n",
      "138\n",
      "138\n",
      "     0    1    2    3    4    5    6    7    8    9 ...    12   13   14   15  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  1.0  0.0   \n",
      "2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
      "\n",
      "    16   17   18   19   20   21  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "                                                text    0    1    2    3    4  \\\n",
      "0  In spite of Jean-Jacques and his school, men a...  0.0  0.0  0.0  0.0  0.0   \n",
      "1  IT was a decided uncompromising rainy day. The...  0.0  0.0  0.0  0.0  0.0   \n",
      "2  THE old Winterbourne house, one of New England...  1.0  0.0  0.0  0.0  0.0   \n",
      "3  THE story contained in this volume is a record...  0.0  0.0  0.0  0.0  0.0   \n",
      "4  This volume is not altogether a military roman...  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "     5    6    7    8 ...    12   13   14   15   16   17   18   19   20   21  \n",
      "0  0.0  0.0  1.0  0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0 ...   0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "4  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "0    1882\n",
      "1    1863\n",
      "2    1910\n",
      "3    1856\n",
      "4    1864\n",
      "Name: year, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/pgrad/wangy12/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#全集的get_data\n",
    "import pandas as pd\n",
    "def get_data_all():\n",
    "    data = pd.read_csv(\"all_ohc.csv\")\n",
    "    data.drop(['Unnamed: 0','book','author'], axis=1,inplace=True)\n",
    "    #print(len(data))\n",
    "    #data.head()\n",
    "    # print(type(train['year'].tolist()))\n",
    "    # print(train['text'])\n",
    "    #print(train.ix[[0],['text']])\n",
    "\n",
    "    x,y = data.ix[:,1:],data.ix[:,0]\n",
    "    #测试集为25%，训练集为75%\n",
    "    train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.25,random_state=0)\n",
    "    train_x = train_x.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    print(len(train_x))\n",
    "    print(len(train_y))\n",
    "    test_x = test_x.reset_index(drop=True)\n",
    "    test_y = test_y.reset_index(drop=True)\n",
    "    print(len(test_x))\n",
    "    print(len(test_y))\n",
    "    return train_x,train_y,test_x,test_y\n",
    "\n",
    "all_train_x,all_train_y,all_test_x,all_test_y = get_data_all()\n",
    "print(all_train_x.iloc[:,1:].head())\n",
    "print(all_train_x.head())\n",
    "print(all_train_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1720\n",
      "8580\n",
      "13360\n",
      "23660\n",
      "17745\n",
      "5915\n",
      "                                                text  age\n",
      "0  Finding a Contractor;;If you decide that will ...    1\n",
      "1  You've got enable you to \"make a website\" 100 ...    3\n",
      "2  <img class=\"alignright\" src=\" ; alt=\"\" width=\"...    3\n",
      "3  These variety from the simplest issues such as...    3\n",
      "4  If you are heading to be in Dubai in transit o...    2\n",
      "17745\n",
      "                                                text  age\n",
      "0  Yahoo Store offers flexibility and maximum pro...    2\n",
      "1  Please help me in studying the complete discou...    3\n",
      "2  Today's customers are witnessing a whole new e...    2\n",
      "3  ; ;;Article by Admin muangchuen at 2012-03-01 ...    3\n",
      "4  used lathes, cnc milling & machine tools onlin...    2\n",
      "5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17745\n",
      "5915\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#全集的get_data\n",
    "def get_data_social():\n",
    "    data = pd.read_csv(\"pan13-author-profiling-training-corpus-2013-01-09/data1.csv\", lineterminator='\\n')\n",
    "    #data = pd.read_csv(\"test_data.csv\", lineterminator='\\n')\n",
    "    data.drop(['Unnamed: 0','Unnamed: 0.1','gender'], axis=1,inplace=True)\n",
    "    #print(len(data))\n",
    "    \n",
    "    data1 = data.loc[data['age_group\\r'] == 1]\n",
    "    data2 = data.loc[data['age_group\\r'] == 2]\n",
    "    data3 = data.loc[data['age_group\\r'] == 3]\n",
    "    # print(len(data1))\n",
    "    # print(len(data2))\n",
    "    # print(len(data3))\n",
    "\n",
    "    rand_arr1 = np.arange(len(data1)*0.1)\n",
    "    np.random.shuffle(rand_arr1)\n",
    "    data11 = data1.iloc[rand_arr1]\n",
    "    print(len(data11))\n",
    "\n",
    "    rand_arr2 = np.arange(len(data2)*0.1)\n",
    "    np.random.shuffle(rand_arr2)\n",
    "    data22 = data2.iloc[rand_arr2]\n",
    "    print(len(data22))\n",
    "\n",
    "    rand_arr3 = np.arange(len(data3)*0.1)\n",
    "    np.random.shuffle(rand_arr3)\n",
    "    data33 = data3.iloc[rand_arr3]\n",
    "    print(len(data33))\n",
    "\n",
    "    data_new = pd.concat([data11,data22,data33])\n",
    "    print(len(data_new))\n",
    "    \n",
    "    data = data_new\n",
    "\n",
    "    x,y = data.ix[:,0],data.ix[:,1]\n",
    "#     print(x.head())\n",
    "#     print(y.head())\n",
    "#     #测试集为25%，训练集为75%\n",
    "    train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.25,random_state=0)\n",
    "    train_x = train_x.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    print(len(train_x))\n",
    "\n",
    "    test_x = test_x.reset_index(drop=True)\n",
    "    test_y = test_y.reset_index(drop=True)\n",
    "    print(len(test_x))\n",
    "    \n",
    "    train_list=[]\n",
    "    for i in list(train_x):\n",
    "        s = i.strip()\n",
    "        i = s\n",
    "        train_list.append(i)\n",
    "    \n",
    "    test_list=[]\n",
    "    for i in list(test_x):\n",
    "        s = i.strip()\n",
    "        i = s\n",
    "        test_list.append(i)\n",
    "    \n",
    "    train_x1 = pd.Series(train_list).to_frame(name=None)\n",
    "    test_x1 = pd.Series(test_list).to_frame(name=None)\n",
    "    \n",
    "    train = pd.concat([train_x1,train_y],axis=1)\n",
    "    test =pd.concat([test_x1,test_y],axis=1)\n",
    "    train.columns = ['text','age']\n",
    "    test.columns = ['text','age']\n",
    "    print(train.head())\n",
    "    print(len(train))\n",
    "    print(test.head())\n",
    "    print(len(test))\n",
    "            \n",
    "    return train_x1,train_y,test_x1,test_y,train,test\n",
    "\n",
    "\n",
    "#     x = x.reset_index(drop=True)\n",
    "#     y = y.reset_index(drop=True)\n",
    "#     print(len(x))\n",
    "#     print(len(y))\n",
    "\n",
    "#     x_list=[]\n",
    "#     for i in list(x):\n",
    "#         s = i.strip()\n",
    "#         i = s\n",
    "#         x_list.append(i)\n",
    "\n",
    "#     x1 = pd.Series(x_list)\n",
    "\n",
    "#     return x1,y\n",
    "\n",
    "#生成social media集的训练集和测试集\n",
    "social_train_x,social_train_y,social_test_x,social_test_y,train,test= get_data_social()\n",
    "# print(social_train_x.head())\n",
    "# print(social_train_y.head())\n",
    "print(len(social_train_x))\n",
    "print(len(social_test_x))\n",
    "# social_x,social_y = get_data_social()\n",
    "# print(len(social_x))\n",
    "\n",
    "train.to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/train.csv\")\n",
    "test.to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=1, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(social_train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4ff79a2d5533>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Unnamed: 0.1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'age_group'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#由于数据量过大，内存首先，随机按年龄比例选取50%，\n",
    "data = pd.read_csv(\"pan13-author-profiling-training-corpus-2013-01-09\\data1.csv\", lineterminator='\\n')\n",
    "\n",
    "data.drop(['Unnamed: 0','Unnamed: 0.1','gender'], axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600\n",
      "42900\n",
      "66800\n",
      "                                                     text  age_group\\r\n",
      "80042   \\r\\t\\t\\tWe produce and export Chemical Product...            1\n",
      "44767   \\r\\t\\t\\tIt is a new year, all of us are just i...            1\n",
      "16944   \\r\\t\\t\\tYou need to take proper care of your s...            1\n",
      "60721   \\r\\t\\t\\tIn this lives, often a lot of the appe...            1\n",
      "62047   \\r\\t\\t\\tThe museum here is open up through the...            1\n",
      "50873   \\r\\t\\t\\they guys im emma cookson, and i sing o...            1\n",
      "117302  \\r\\t\\t\\tThe furniture is usually the first thi...            1\n",
      "73751   \\r\\t\\t\\tMany students and I took part in the t...            1\n",
      "14030   \\r\\t\\t\\tOrder wine by the glass, not the bottl...            1\n",
      "60065   \\r\\t\\t\\tWhen you look at how many plus size <a...            1\n",
      "23054   \\r\\t\\t\\tMany banks and finance companies offer...            1\n",
      "59252   \\r\\t\\t\\tToday, most house painting paints is n...            1\n",
      "80204   \\r\\t\\t\\tIf you locate yourself hemming and haw...            1\n",
      "53912   \\r\\t\\t\\tWhat is too much for one business owne...            1\n",
      "103631  \\r\\t\\t\\tBuying real estate is quite an underta...            1\n",
      "48893   \\r\\t\\t\\t Bookmakers receive bets on sporting a...            1\n",
      "2421    \\r\\t\\t\\t(It might also curb low-growing weeds ...            1\n",
      "77266   \\r\\t\\t\\t;The one thing that makes a <a alt=\"ug...            1\n",
      "31151   \\r\\t\\t\\tIt seems odd for some of us, but as de...            1\n",
      "19152   \\r\\t\\t\\tMall operators be contingent on attrac...            1\n",
      "3462    \\r\\t\\t\\tIf the comments is adverse then you ca...            1\n",
      "66667   \\r\\t\\t\\tEach day it appears like there are muc...            1\n",
      "89812   \\r\\t\\t\\twelcome i don't know how to make frien...            1\n",
      "22161   \\r\\t\\t\\tThe actual Infinity auto insurance pro...            1\n",
      "66372   \\r\\t\\t\\tWedding are unique! Two turning into a...            1\n",
      "36636   \\r\\t\\t\\tAnybody who's been to a ceremony in ;t...            1\n",
      "96478   \\r\\t\\t\\tIt really is crucial nowadays to have ...            1\n",
      "6046    \\r\\t\\t\\tHey guys pls join this cool groups in ...            1\n",
      "76307   \\r\\t\\t\\tThere are places that spend a lot of m...            1\n",
      "36115   \\r\\t\\t\\tHi babes hru all? this is my URL: ; ;f...            1\n",
      "...                                                   ...          ...\n",
      "12126   \\r\\t\\t\\tA great deal of people think that a ve...            3\n",
      "12750   \\r\\t\\t\\tIt seems like we’re always running aro...            3\n",
      "52362   \\r\\t\\t\\tVisiting a wine tasting get together c...            3\n",
      "65480   \\r\\t\\t\\tMaybe you are like us in that you feel...            3\n",
      "81485   \\r\\t\\t\\tHave you ever considered how you would...            3\n",
      "2067    \\r\\t\\t\\t; ; ; that you're 5 months pregnant, y...            3\n",
      "112806  \\r\\t\\t\\tFor high quality, cutting edge technol...            3\n",
      "54276   \\r\\t\\t\\tI did not want to eliminate her and as...            3\n",
      "81762   \\r\\t\\t\\tAre you a user of Slendertone and are ...            3\n",
      "55661   \\r\\t\\t\\tThis is a great quality of their equip...            3\n",
      "104506  \\r\\t\\t\\tJust when you think you've got the lat...            3\n",
      "53724   \\r\\t\\t\\tI feel alive,now i can breath again,i ...            3\n",
      "90252   \\r\\t\\t\\t[right]im an outgoing person who likes...            3\n",
      "96113   \\r\\t\\t\\tTo ensure that you can have the best v...            3\n",
      "65375   \\r\\t\\t\\tEducation is everything: the key to in...            3\n",
      "45562   \\r\\t\\t\\tWhen you entirely discover the lesson ...            3\n",
      "74875   \\r\\t\\t\\tEach and every year, the value of like...            3\n",
      "94359   \\r\\t\\t\\tAt home hair colour is actually a mill...            3\n",
      "166     \\r\\t\\t\\t a step-by-phase video clip instructio...            3\n",
      "89498   \\r\\t\\t\\tMany individuals really look no furthe...            3\n",
      "85383   \\r\\t\\t\\tI suppose that?s the location where th...            3\n",
      "86813   \\r\\t\\t\\tHi friends !;;we are at lahore realy i...            3\n",
      "99570   \\r\\t\\t\\tFor those who work in the taste busine...            3\n",
      "21252   \\r\\t\\t\\tWide format printers are those that ha...            3\n",
      "40288   \\r\\t\\t\\tA good objective of leadership is to h...            3\n",
      "83086   \\r\\t\\t\\tIf you are looking for a free advertin...            3\n",
      "36227   \\r\\t\\t\\tBMW Scanner 1 version;you should Downl...            3\n",
      "9402    \\r\\t\\t\\tNails have no Nike Free color and are ...            3\n",
      "90886   \\r\\t\\t\\tNext on my checklist was breast reduct...            3\n",
      "30276   \\r\\t\\t\\tDiapers and formula are often the 2 la...            3\n",
      "\n",
      "[118300 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data1 = data.loc[data['age_group\\r'] == 1]\n",
    "data2 = data.loc[data['age_group\\r'] == 2]\n",
    "data3 = data.loc[data['age_group\\r'] == 3]\n",
    "# print(len(data1))\n",
    "# print(len(data2))\n",
    "# print(len(data3))\n",
    "\n",
    "rand_arr1 = np.arange(len(data1)/2)\n",
    "np.random.shuffle(rand_arr1)\n",
    "data11 = data1.iloc[rand_arr1]\n",
    "print(len(data11))\n",
    "\n",
    "rand_arr2 = np.arange(len(data2)/2)\n",
    "np.random.shuffle(rand_arr2)\n",
    "data22 = data2.iloc[rand_arr2]\n",
    "print(len(data22))\n",
    "\n",
    "rand_arr3 = np.arange(len(data3)/2)\n",
    "np.random.shuffle(rand_arr3)\n",
    "data33 = data3.iloc[rand_arr3]\n",
    "print(len(data33))\n",
    "\n",
    "data_new = pd.concat([data11,data22,data33])\n",
    "print(len(data_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.特征提取：word分词，stem，pos，pos+word。---都可以以jason形式保存，随时提取（保存好）\n",
    "1）先分词，生成list\n",
    "2）用nltk讲word list变成stem，pos，（pos，word）形式\n",
    "3）保存各自的list就是该feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.生成矩阵:!!注意，train，validation和test都是各自生成了各自的feature，只是方法一样，变成了三个对应的矩阵而已！！\n",
    "（先分好的train，validation,test分别返回，参见get_data函数（对应取用哪个feature type）），对应的csv，也有对应的y值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.应用linear regression--用sklearn，将预测结果保存成csv(只有两列，真实 结果和预测结果）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.分析线性相关性的因素，从3种feature各自分析，哪些特征是最相关的，并且呈现什么变化（通过统计方法，x值和y值做图说明seaborn可以说明）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Another way of receiving cosmetology education is using the distance learning with the assistance of which people may research beauty therapy or receive qualification via a reflexology or maybe massage course. ;;Let us take a look at the beauty therapy college course more thoroughly. While covering this training manual an individual will get much information about products, equipment and treatment plans. The beauty therapy college course will supply a student with the company side of the beauty therapy industry not to mention with the principles in its work. It might possibly be just perfect to include the course successfully, when the successful completion of the beauty therapy college course may easily cause the great possibility to operate as a trainee charm therapist or receive another job. ;;You should decide what career you desire to choose - the career of your beauty therapist and on the hair dresser. It is important to mention there\\'s a considerable difference around beauty therapy and hair dressing. And it is especially vital that you understand that difference initially for those people who are willing to make natural beauty their future profession. Beauty courses efficiently influence a man or woman willing to enjoy true peace inside their souls and minds. The peace, undoubtedly, may be visualized over a person\\'s appearance. Probably, right here is the major reason why a smartly designed therapy program consists associated with aroma therapy and stress reduction services for instance various massage techniques. These techniques are able to relax an individual don\\'t just physically but also at your inner levels. The matter is that her person with tense muscular tissues doesn\\'t look attractive, especially when the reason of this tenseness may be a nervous stress. The human mind at the moment is often distracted from the modern quickly changing life-style. Another reason for shedding of attractiveness is the aging process. The matter is that aging influences the way somebody feels inside at 1st, only then it is reflected on the external appearance of somebody. As a consequence, beauty therapy concentrates on designing the appropriate surrounding needed for a personal transformation. ;;It is not with such ease to get an education in neuro-scientific beauty therapy. The matter is that process of obtaining some sort of beauty therapy certificate requires various various courses which are specially which will teach, guide and improve the ability of a future beauty therapist just to be completely prepared to improve a person\\'s appearance. These specific courses will teach individuals not only different ways of applying facials or generating an aroma therapeutic associated with, but give the fundamental information about how food and nutrition may well be connected with a person\\'s visual appearance, health and overall health of a patient. Your immediate future specialist will learn interesting and useful the informatioin needed for cosmetic chemistry, thus having the specialty more thin including more hair-coloring, face-coloring as well as skin. In addition, an individual will be able to study the essentials of salon management and also learn the ways of handling both business ability and personal care into the perfect business model. ;;And what is crucial about the beauty remedies courses is that using this method of studying a person offers an excellent opportunity to get accustomed to the general sphere of your beauty field and receive enough experience to try and force a reputation of a superb beauty professional. ; ,  ; ,  ;\\r\\t\\t\\r\\t\\t\\tIt can also be suggested if anyone else is of all age groups, which are from teenagers to even the aged. Thus, the uses of acupuncture attended into serious attention and have become a very powerful remedy of today\\'s life. ; acupuncture has gained increasing acceptance internationally, a number of concerns concerning its origins remain unanswered. Acupuncture is generally often called an Eastern system of healing that started in China. However, it has also been suggested that acupuncture often have first arisen in India or maybe Central Europex. In this post we will discuss parallels between theory and practice of acupuncture and then the development of flood deal with in ancient China, which support concluding that acupuncture is your uniquely Chinese invention. ;;The techniques and vocabulary of flood control have a vivid analogy of the actual therapeutic mechanisms of acupuncture. The meridians in the body are seen to correspond to the river courses in the earth, channeling the qi and additionally blood which nourish the actual tissues, just as any rivers\\' waters irrigate all the land. Blockages in these \"energy rivers\" stand for dams, obstructing the flow of qi and blood and causing it to back up in connecting channels. Needling that acupuncture points removes the particular obstructions, curing disease by reestablishing the standard flow of qi plus blood, just as dredging a good river clears away sediment, preventing flooding by allowing the to flow freely. ;;These and similar descriptions happen to be applied to acupuncture the way it first appeared as a complete system of healing ahead of time in China\\'s Western Han Empire (206 BC-24 AD). Many examples could possibly be found in the Neijing (c. 104-32 BC), the oldest extant develop acupuncture. Such hydraulic terminology was employed not only for its evocative photos. Rather, it indicates the recognizing the Chinese ancestors had attained by this period of the correspondences around Nature and Human, riv and meridian, flood and disease. The development involving flood control in Chinese suppliers, which reached its peak late inside Warring States Period (476-221 BC), and the application of its principles to the healing of our bodies, was an essential precondition for your invention for acupuncture (4). That context thus established appeared to be unique to China, and couldn\\'t exist in India, Fundamental Europe, or anywhere else while in the classical world. ;;Water can capsize a boat, and also float it. The dual nature in water - its ability to harm combined with help - has driven humanity to position great effort into harnessing it\\'s power. The history of innundate control in China is provided that of Chinese civilization. The ancient Chinese who lived while in the Yellow River and Changjiang (Yangtze) Riv basins regularly experienced considerable and protracted flooding, in addition to benefits of these waters. After emerging from a gorge, the Inner River continues for several hundred kilometers, branching out into numerous irrigation signals until it finally carries on out. Flying Sand Weir, a lowered section of the bifurcating dam, is at shortly before Precious Urn Outlet and serves as a spillway in times associated with high water. It diverts excess water from your Inner River to this Outer River, preventing it from keying in Precious Vase Outlet therefore preventing flooding. ;  ; ,  ;'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(social_train_x)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      In spite of Jean-Jacques and his school, men a...\n",
       "1      IT was a decided uncompromising rainy day. The...\n",
       "2      THE old Winterbourne house, one of New England...\n",
       "3      THE story contained in this volume is a record...\n",
       "4      This volume is not altogether a military roman...\n",
       "5      By compulsion emigrated from Sedgemoor, and fo...\n",
       "6      “Snug-Harbor” is the second volume of “THE BOA...\n",
       "7      The author has taken a few liberties with the ...\n",
       "8      The ideal is the essence of poetry. In the vir...\n",
       "9      I When the murder was done and the heralds sho...\n",
       "10     It was in 1590--winter. Austria was far away f...\n",
       "11     Ralph Destournier went gayly along, whistling ...\n",
       "12     'THEN you think I ought not speak of this to K...\n",
       "13     It was my original intention to write a biogra...\n",
       "14     It had been a great day for the children at Ho...\n",
       "15     The first created thing was light. Then life c...\n",
       "16     Among the most interesting and picturesque cla...\n",
       "17     \"FIGHTING FOR THE RIGHT\" is the fifth and last...\n",
       "18     Aristides Homos, an Emissary of the Altrurian ...\n",
       "19     It all depends upon the manner of your entranc...\n",
       "20     The Leveretts were at their breakfast in the l...\n",
       "21     The hall of the banquets was made ready for th...\n",
       "22     \"You may stay down here until nine o’clock if ...\n",
       "23     Amelia Maxwell sat by the front-chamber window...\n",
       "24     There could not have been a more sympathetic m...\n",
       "25     \"A letter for me, did you say?\" The speaker wa...\n",
       "26     |You’re a bad lot, Bernard Brooks. I don’t thi...\n",
       "27     Some one asked me the other day, if I were not...\n",
       "28     Carlo Zeno, gentleman of Venice, ex-clerk, ex-...\n",
       "29     A dozen men, provided with rockers, were busil...\n",
       "                             ...                        \n",
       "384    In this volume my friend Captain Galligasken h...\n",
       "385    Though it is an exaggeration to say that there...\n",
       "386    What when of lives that, reaching up to heaven...\n",
       "387    ROLDAN CASTANADA walked excitedly up and down ...\n",
       "388    I AM telling the story of Sylvia Castleman. I ...\n",
       "389    However much the author of \"WATCH AND WAIT\" ma...\n",
       "390    Khaled stood in the third heaven, which is the...\n",
       "391    During the passage of this story through THE C...\n",
       "392    OLIVE will come down in about ten minutes she ...\n",
       "393    Out of the unromantic night, out of the somber...\n",
       "394    THE title of this volume sufficiently indicate...\n",
       "395    Two years of service in the Zouaves had wrough...\n",
       "396    'I cannot help it,' said Filmore Durand quietl...\n",
       "397    \"Royal,\" said the man's mother that evening, \"...\n",
       "398    About noon of a day in May during the recent y...\n",
       "399    LIFE is a mystery to all men, and the more pro...\n",
       "400    WAS a Fairharbor boy. This might be to say any...\n",
       "401    My dear Mr. Alden,—It was at your suggestion t...\n",
       "402    \"All Adrift\" is the first volume of a new set ...\n",
       "403    There is a ruby mine hidden in the heart of th...\n",
       "404    It is of no consequence why or how we came to ...\n",
       "405    If you looked at the mountain from the west, t...\n",
       "406    In the good old times, when \"Little Women\" wor...\n",
       "407    CHAPTER I. Far away on a green hill-side, a li...\n",
       "408    In the best room of a farm-house on the skirts...\n",
       "409    When I consented to prepare this volume for a ...\n",
       "410    \"I say, you know, Kate--you did stay!\" had bee...\n",
       "411    When this story begins, the youngest of those ...\n",
       "412    It appears to the writer that there is urgent ...\n",
       "413    IF this little book reads more like a memoir t...\n",
       "Name: text, Length: 414, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_x['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成3种类型的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# f = open(\"t.txt\",'rb')\n",
    "# text =  f.read()\n",
    "# text = bytes.decode(text)\n",
    "# f.close()\n",
    "#text = \"ALONG stretch of level country, bordered on one side by the sea, sweeps away until the imagination begins its guesswork where the eye is forced to stop, at a thick fringe of pines in the far background. Here and there, perhaps two miles from the sea, are farm houses in picturesque groups of threes and fours, forming the little town of StratfordbytheSea. This quaint corner of the old township dates back to colonial days, and almost to colonial primitiveness of life, though, like many another place and people, it has retained certain forms and customs of its ancestors without their accompanying rigidity of virtue. But, alas ! true as this is in the main, many a good custom is unhappily obsolete ; there are scolds in Stratford who have made no acquaintance with the ducking stool, and an occasional worthy member of society dares absent himself from church without fear of a penalty.\"\n",
    "\n",
    "#去除停用词\n",
    "\n",
    "#分词\n",
    "def word_feature(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    word_list = list(set(word_list))\n",
    "    return word_list\n",
    "\n",
    "#print(word_feature(text))\n",
    "\n",
    "\"\"\"\n",
    "#feature1：生成stem\n",
    "def stem_feature(text):\n",
    "    word_list = word_feature(text)\n",
    "    stem_list = []\n",
    "    p = PorterStemmer()\n",
    "    for word in word_list:\n",
    "        stem = p.stem(word)\n",
    "        stem_list.append(stem)\n",
    "\n",
    "    stem_list = list(set(stem_list))    \n",
    "    return stem_list\n",
    "\n",
    "print(stem_feature(text))\n",
    "\"\"\"\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "#feature1:生成lemma\n",
    "def lemma_feature(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_list = word_feature(text)\n",
    "    lemma_list = []\n",
    "    for word in word_list:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "        lemma_list.append(lemma)\n",
    "\n",
    "    lemma_list1 = list(set(lemma_list))\n",
    "    \n",
    "    lemma_list = [x.lower() for x in lemma_list1 if isinstance(x,str)] \n",
    "    return lemma_list\n",
    "\n",
    "\n",
    "\n",
    "#feature2：生成pos\n",
    "def pos_feature(text):\n",
    "    word_list = word_feature(text)\n",
    "    pos_list = []\n",
    "    lexical_list = nltk.pos_tag(word_list)\n",
    "    for pos in lexical_list:\n",
    "        pos_list.append(pos[1])    \n",
    "    \n",
    "    pos_list = list(set(pos_list))\n",
    "    return pos_list\n",
    "\n",
    "\n",
    "\n",
    "#feature3:生成lexical（lemma+pos）\n",
    "def lexical_feature(text):\n",
    "    lemma_list = word_feature(text)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lexical_list = nltk.pos_tag(lemma_list)\n",
    "    lexical_list1 = []\n",
    "    for i in lexical_list:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(i[0], get_wordnet_pos(i[0]))\n",
    "        if isinstance(lemma,str):\n",
    "             lemma.lower()\n",
    "        temp =lemma+'.'+i[1]\n",
    "        lexical_list1.append(temp)\n",
    "    \n",
    "    lexical_list1 = list(set(lexical_list1))\n",
    "    return lexical_list1\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##提取train 集特征值\n",
    "#1.lemma特征值\n",
    "def lemma_f(train_x):\n",
    "    lemma_f = []\n",
    "    i = 0\n",
    "    for text in train_x:\n",
    "        i = i+1\n",
    "        temp_lemma_list = lemma_feature(text)\n",
    "        lemma_f = list(set(lemma_f).union(set(temp_lemma_list)))\n",
    "        if i%50==0:\n",
    "            print(\"lemma_finished:\"+str(i)+','+str(i/len(train_x))+\"....\")\n",
    "#     print(lemma_f)\n",
    "#     print(len(lemma_f))\n",
    "    return lemma_f\n",
    "  \n",
    "    \n",
    "#2.pos特征值\n",
    "def pos_f(train_x):\n",
    "    pos_f = []\n",
    "    i = 0\n",
    "    for text in train_x:\n",
    "        i = i+1\n",
    "        #print(i)\n",
    "        temp_pos_list = pos_feature(text)\n",
    "        pos_f = list(set(pos_f).union(set(temp_pos_list)))\n",
    "        if i%50==0:\n",
    "            print(\"pos_finished:\"+str(i)+','+str(i/len(train_x))+\"....\")\n",
    "    return pos_f\n",
    "\n",
    "\n",
    "#3.lexical特征值\n",
    "def lexical_f(train_x):\n",
    "    lexical_f = []\n",
    "    i = 0\n",
    "    for text in train_x:\n",
    "        i = i+1\n",
    "        temp_lexical_list = lexical_feature(text)\n",
    "        lexical_f = list(set(lexical_f).union(set(temp_lexical_list)))\n",
    "        if i%50==0:\n",
    "            print(\"lexical_finished:\"+str(i)+','+str(i/len(train_x))+\"....\")\n",
    "    return lexical_f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "finished:5....\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#生成22个作家各自训练集的lemma特征值\n",
    "#ab_lemma_f = lemma_f(ab_train_x)\n",
    "\n",
    "#生成22个作家各自训练集的pos特征值\n",
    "ab_pos_f = pos_f(ab_train_x)\n",
    "\n",
    "# #生成22个作家各自训练集的pos特征值\n",
    "# ab_lexical_f = lexical_f(ab_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /users/pgrad/wangy12/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "finished:5....\n",
      "finished:10....\n",
      "finished:15....\n",
      "finished:20....\n",
      "finished:25....\n",
      "finished:30....\n",
      "finished:35....\n",
      "finished:40....\n",
      "finished:45....\n",
      "finished:50....\n",
      "finished:55....\n",
      "finished:60....\n",
      "finished:65....\n",
      "finished:70....\n",
      "finished:75....\n",
      "finished:80....\n",
      "finished:85....\n",
      "finished:90....\n",
      "finished:95....\n",
      "finished:100....\n",
      "finished:105....\n",
      "finished:110....\n",
      "finished:115....\n",
      "finished:120....\n",
      "finished:125....\n",
      "finished:130....\n",
      "finished:135....\n",
      "finished:140....\n",
      "finished:145....\n",
      "finished:150....\n",
      "finished:155....\n",
      "finished:160....\n",
      "finished:165....\n",
      "finished:170....\n",
      "finished:175....\n",
      "finished:180....\n",
      "finished:185....\n",
      "finished:190....\n",
      "finished:195....\n",
      "finished:200....\n",
      "finished:205....\n",
      "finished:210....\n",
      "finished:215....\n",
      "finished:220....\n",
      "finished:225....\n",
      "finished:230....\n",
      "finished:235....\n",
      "finished:240....\n",
      "finished:245....\n",
      "finished:250....\n",
      "finished:255....\n",
      "finished:260....\n",
      "finished:265....\n",
      "finished:270....\n",
      "finished:275....\n",
      "finished:280....\n",
      "finished:285....\n",
      "finished:290....\n",
      "finished:295....\n",
      "finished:300....\n",
      "finished:305....\n",
      "finished:310....\n",
      "finished:315....\n",
      "finished:320....\n",
      "finished:325....\n",
      "finished:330....\n",
      "finished:335....\n",
      "finished:340....\n",
      "finished:345....\n",
      "finished:350....\n",
      "finished:355....\n",
      "finished:360....\n",
      "finished:365....\n",
      "finished:370....\n",
      "finished:375....\n",
      "finished:380....\n",
      "finished:385....\n",
      "finished:390....\n",
      "finished:395....\n",
      "finished:400....\n",
      "finished:405....\n",
      "finished:410....\n",
      "finished:5....\n",
      "finished:10....\n",
      "finished:15....\n",
      "finished:20....\n",
      "finished:25....\n",
      "finished:30....\n",
      "finished:35....\n",
      "finished:40....\n",
      "finished:45....\n",
      "finished:50....\n",
      "finished:55....\n",
      "finished:60....\n",
      "finished:65....\n",
      "finished:70....\n",
      "finished:75....\n",
      "finished:80....\n",
      "finished:85....\n",
      "finished:90....\n",
      "finished:95....\n",
      "finished:100....\n",
      "finished:105....\n",
      "finished:110....\n",
      "finished:115....\n",
      "finished:120....\n",
      "finished:125....\n",
      "finished:130....\n",
      "finished:135....\n",
      "finished:140....\n",
      "finished:145....\n",
      "finished:150....\n",
      "finished:155....\n",
      "finished:160....\n",
      "finished:165....\n",
      "finished:170....\n",
      "finished:175....\n",
      "finished:180....\n",
      "finished:185....\n",
      "finished:190....\n",
      "finished:195....\n",
      "finished:200....\n",
      "finished:205....\n",
      "finished:210....\n",
      "finished:215....\n",
      "finished:220....\n",
      "finished:225....\n",
      "finished:230....\n",
      "finished:235....\n",
      "finished:240....\n",
      "finished:245....\n",
      "finished:250....\n",
      "finished:255....\n",
      "finished:260....\n",
      "finished:265....\n",
      "finished:270....\n",
      "finished:275....\n",
      "finished:280....\n",
      "finished:285....\n",
      "finished:290....\n",
      "finished:295....\n",
      "finished:300....\n",
      "finished:305....\n",
      "finished:310....\n",
      "finished:315....\n",
      "finished:320....\n",
      "finished:325....\n",
      "finished:330....\n",
      "finished:335....\n",
      "finished:340....\n",
      "finished:345....\n",
      "finished:350....\n",
      "finished:355....\n",
      "finished:360....\n",
      "finished:365....\n",
      "finished:370....\n",
      "finished:375....\n",
      "finished:380....\n",
      "finished:385....\n",
      "finished:390....\n",
      "finished:395....\n",
      "finished:400....\n",
      "finished:405....\n",
      "finished:410....\n",
      "finished:5....\n",
      "finished:10....\n",
      "finished:15....\n",
      "finished:20....\n",
      "finished:25....\n",
      "finished:30....\n",
      "finished:35....\n",
      "finished:40....\n",
      "finished:45....\n",
      "finished:50....\n",
      "finished:55....\n",
      "finished:60....\n",
      "finished:65....\n",
      "finished:70....\n",
      "finished:75....\n",
      "finished:80....\n",
      "finished:85....\n",
      "finished:90....\n",
      "finished:95....\n",
      "finished:100....\n",
      "finished:105....\n",
      "finished:110....\n",
      "finished:115....\n",
      "finished:120....\n",
      "finished:125....\n",
      "finished:130....\n",
      "finished:135....\n",
      "finished:140....\n",
      "finished:145....\n",
      "finished:150....\n",
      "finished:155....\n",
      "finished:160....\n",
      "finished:165....\n",
      "finished:170....\n",
      "finished:175....\n",
      "finished:180....\n",
      "finished:185....\n",
      "finished:190....\n",
      "finished:195....\n",
      "finished:200....\n",
      "finished:205....\n",
      "finished:210....\n",
      "finished:215....\n",
      "finished:220....\n",
      "finished:225....\n",
      "finished:230....\n",
      "finished:235....\n",
      "finished:240....\n",
      "finished:245....\n",
      "finished:250....\n",
      "finished:255....\n",
      "finished:260....\n",
      "finished:265....\n",
      "finished:270....\n",
      "finished:275....\n",
      "finished:280....\n",
      "finished:285....\n",
      "finished:290....\n",
      "finished:295....\n",
      "finished:300....\n",
      "finished:305....\n",
      "finished:310....\n",
      "finished:315....\n",
      "finished:320....\n",
      "finished:325....\n",
      "finished:330....\n",
      "finished:335....\n",
      "finished:340....\n",
      "finished:345....\n",
      "finished:350....\n",
      "finished:355....\n",
      "finished:360....\n",
      "finished:365....\n",
      "finished:370....\n",
      "finished:375....\n",
      "finished:380....\n",
      "finished:385....\n",
      "finished:390....\n",
      "finished:395....\n",
      "finished:400....\n",
      "finished:405....\n",
      "finished:410....\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#生成全集合的lemma特征值\n",
    "all_lemma_f = lemma_f(all_train_x['text'])\n",
    "\n",
    "#生成全集合的lemma特征值\n",
    "all_pos_f = pos_f(all_train_x['text'])\n",
    "\n",
    "#生成全集合的lemma特征值\n",
    "all_lexical_f = lexical_f(all_train_x['text'])\n",
    "\n",
    "#将全集特征写入json文件\n",
    "data_dict = {'lemma':all_lemma_f,'pos':all_pos_f,'lexical':all_lexical_f}\n",
    "json_str = json.dumps(data_dict)\n",
    "with open('all_feature.json', 'w') as f:\n",
    "    f.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'EX', 'CD', 'FW', 'NNS', 'NNP', 'POS', 'WP', 'VBD', 'RB', '#', 'RP', 'WDT', 'JJR', 'TO', 'JJ', 'CC', 'VB', 'MD', 'DT', 'VBG', '.', 'IN', \"''\", 'RBS', 'WRB', 'VBP', 'VBZ', 'VBN', 'NN', 'UH', 'PDT', 'RBR', 'PRP', 'JJS', ',', 'WP$', ':', 'NNPS', 'SYM', '$', '``', 'PRP$', ')']\n",
      "191806\n",
      "44\n",
      "321725\n"
     ]
    }
   ],
   "source": [
    "#提取all集的特征值\n",
    "with open('all_feature.json', encoding='utf-8') as data_file:\n",
    "    features = json.loads(data_file.read())\n",
    "\n",
    "print(features['pos'])\n",
    "all_lemma_f = features['lemma']\n",
    "all_pos_f = features['pos']\n",
    "all_lexical_f = features['lexical']\n",
    "print(len(all_lemma_f))\n",
    "print(len(all_pos_f))\n",
    "print(len(all_lexical_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\qxy09\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b1e3a9231a43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'averaged_perceptron_tagger'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msocial_lemma_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemma_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msocial_pos_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msocial_lexical_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexical_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#生成social集的三种特征集\n",
    "import json\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "social_lemma_f = lemma_f(train['text'])\n",
    "social_pos_f = pos_f(train['text'])\n",
    "social_lexical_f = lexical_f(train['text'])\n",
    "\n",
    "#将social集特征写入jason文件\n",
    "data_dict2 = {'social_lemma':social_lemma_f,'social_pos':social_pos_f,'social_lexical':social_lexical_f}\n",
    "json_str2 = json.dumps(data_dict2)\n",
    "with open('social_feature3.json', 'w') as f:\n",
    "    f.write(json_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', '#', ',', 'FW', 'IN', 'JJS', 'RBS', 'PRP$', 'VBN', '``', 'VBZ', 'JJ', 'WRB', 'RP', '$', 'SYM', 'VBG', 'RBR', 'NNP', '.', 'RB', 'NN', 'CD', 'CC', ':', 'PRP', \"''\", 'NNS', 'WP', 'VB', 'VBD', '(', 'UH', 'VBP', 'POS', 'JJR', 'WP$', 'PDT', 'EX', 'DT', 'TO', 'LS', 'NNPS', 'MD', 'WDT']\n",
      "137223\n",
      "45\n",
      "257716\n"
     ]
    }
   ],
   "source": [
    "#提取social集的特征值\n",
    "with open('social_feature3.json', encoding='utf-8') as data_file:\n",
    "    features = json.loads(data_file.read())\n",
    "#print(features['social_lemma'])\n",
    "print(features['social_pos'])\n",
    "social_lemma_f = features['social_lemma']\n",
    "social_pos_f = features['social_pos']\n",
    "social_lexical_f = features['social_lexical']\n",
    "print(len(social_lemma_f))\n",
    "print(len(social_pos_f))\n",
    "print(len(social_lexical_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train和test set 的x分别向量化\n",
    "def vectorize(sent,feature_list):\n",
    "    vector = [sent.count(f) for f in feature_list]\n",
    "    return(vector)\n",
    "#print(vectorize(['LI', 'hurrying', 'beckoning', 'beneficent', 'Scholar', 'rarely', 'faltering'],ab_lemma_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-42f09b3cd3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#生成22个作家对应的lemma,pos,lexical分别的train和test向量化后的输入\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#df = create_matrix(ab_train_x,ab_train_y,ab_stem_f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcreate_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab_train_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mab_train_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlemma_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mab_lemma_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DCLSA/la_dataset_split/ab/ab_lemma_train_matrix.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mcreate_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab_test_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mab_test_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlemma_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mab_lemma_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DCLSA/la_dataset_split/ab/ab_lemma_test_matrix.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcreate_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab_train_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mab_train_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mab_pos_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DCLSA/la_dataset_split/ab/ab_pos_train_matrix.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_train_x' is not defined"
     ]
    }
   ],
   "source": [
    "#train和test set 的文本向量化\n",
    "def create_matrix(x,y,feature_fun,feature_type):\n",
    "#     \"Creates the apporiate feature matrix\"\n",
    "#     mat = pd.read_csv(\"{}.csv\".format(name))\n",
    "#     mat.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "    \n",
    "    origin_list = x\n",
    "    feature_list = list(map(feature_fun,origin_list))\n",
    "    \n",
    "    vectors = [vectorize(s,feature_type) for s in feature_list]\n",
    "    df = pd.DataFrame(vectors, columns=feature_type)\n",
    "    \n",
    "    df['YEAR'] = y\n",
    "    print(y)\n",
    "    print(df['YEAR'])\n",
    "    #df.to_csv(\"test_matrix.csv\")\n",
    "    return df\n",
    "\n",
    "#生成22个作家对应的lemma,pos,lexical分别的train和test向量化后的输入\n",
    "#df = create_matrix(ab_train_x,ab_train_y,ab_stem_f)\n",
    "create_matrix(ab_train_x,ab_train_y,lemma_feature,ab_lemma_f).to_csv(\"DCLSA/la_dataset_split/ab/ab_lemma_train_matrix.csv\")\n",
    "create_matrix(ab_test_x,ab_test_y,lemma_feature,ab_lemma_f).to_csv(\"DCLSA/la_dataset_split/ab/ab_lemma_test_matrix.csv\")\n",
    "create_matrix(ab_train_x,ab_train_y,pos_feature,ab_pos_f).to_csv(\"DCLSA/la_dataset_split/ab/ab_pos_train_matrix.csv\")\n",
    "create_matrix(ab_test_x,ab_test_y,pos_feature,ab_pos_f).to_csv(\"DCLSA/la_dataset_split/ab/ab_pos_test_matrix.csv\")\n",
    "create_matrix(ab_train_x,ab_train_y,lexical_feature,ab_lexical_f).to_csv(\"DCLSA/la_dataset_split/ab/ab_lexical_train_matrix.csv\")\n",
    "create_matrix(ab_test_x,ab_test_y,lexical_feature,ab_lexical_f).to_csv(\"DCLSA/la_dataset_split/ab/ab_lexcial_test_matrix.csv\")\n",
    "\n",
    "# ab_train_matrix.csv\n",
    "# ab_test_matrix.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>shack.RB</th>\n",
       "      <th>belt.VBD</th>\n",
       "      <th>blinds.NNS</th>\n",
       "      <th>polite.NN</th>\n",
       "      <th>foothold.VBP</th>\n",
       "      <th>irritably.RB</th>\n",
       "      <th>intended.VBN</th>\n",
       "      <th>Rhadamanthus.NNP</th>\n",
       "      <th>adjoyning.VBG</th>\n",
       "      <th>...</th>\n",
       "      <th>jeweled.VBN</th>\n",
       "      <th>lean.JJ</th>\n",
       "      <th>self-respecting.NN</th>\n",
       "      <th>splendid.JJ</th>\n",
       "      <th>well-beloved.JJ</th>\n",
       "      <th>throwing.VBG</th>\n",
       "      <th>Ai.NNP</th>\n",
       "      <th>Baptist.NNP</th>\n",
       "      <th>water-course.JJ</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  shack.RB  belt.VBD  blinds.NNS  polite.NN  foothold.VBP  \\\n",
       "0           0         0         0           0          0             0   \n",
       "1           1         0         0           1          0             0   \n",
       "2           2         0         0           1          0             0   \n",
       "\n",
       "   irritably.RB  intended.VBN  Rhadamanthus.NNP  adjoyning.VBG  ...   \\\n",
       "0             1             1                 0              0  ...    \n",
       "1             1             1                 0              0  ...    \n",
       "2             0             1                 0              0  ...    \n",
       "\n",
       "   jeweled.VBN  lean.JJ  self-respecting.NN  splendid.JJ  well-beloved.JJ  \\\n",
       "0            0        1                   0            1                0   \n",
       "1            0        1                   0            1                1   \n",
       "2            0        1                   0            0                0   \n",
       "\n",
       "   throwing.VBG  Ai.NNP  Baptist.NNP  water-course.JJ  YEAR  \n",
       "0             1       1            0                0  1910  \n",
       "1             1       1            0                0  1922  \n",
       "2             1       1            0                0  1899  \n",
       "\n",
       "[3 rows x 36460 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('DCLSA/la_dataset_split/ab/ab_lexcial_test_matrix.csv')\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全集的向量化（多了一个author的元素）\n",
    "def create_matrix_all(x,y,feature_fun,feature_type):\n",
    "#def create_matrix_all(x,y):\n",
    "    #文本变量向量化\n",
    "    test_list = x['text']\n",
    "    feature_list = list(map(feature_fun,test_list))\n",
    "    \n",
    "    vectors = [vectorize(s,feature_type) for s in feature_list]\n",
    "    df1 = pd.DataFrame(vectors, columns=feature_type)\n",
    "    df2 = x.iloc[:,1:]\n",
    "    \n",
    "    df = pd.concat([df1,df2],axis=1)\n",
    "    df['YEAR'] = y\n",
    "    return df\n",
    "\n",
    "\n",
    "# create_matrix_all(all_train_x,all_train_y,lemma_feature,all_lemma_f).to_csv(\"all_lemma_train_matrix.csv\")\n",
    "# create_matrix_all(all_test_x,all_test_y,lemma_feature,all_lemma_f).to_csv(\"all_lemma_test_matrix.csv\")\n",
    "# create_matrix_all(all_train_x,all_train_y,pos_feature,all_pos_f).to_csv(\"all_pos_train_matrix.csv\")\n",
    "# create_matrix_all(all_test_x,all_test_y,pos_feature,all_pos_f).to_csv(\"all_pos_test_matrix.csv\")\n",
    "# create_matrix_all(all_train_x,all_train_y,lexical_feature,all_lexical_f).to_csv(\"all_lexical_train_matrix.csv\")\n",
    "# create_matrix_all(all_test_x,all_test_y,lexical_feature,all_lexical_f).to_csv(\"all_lexcial_test_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#social集的矩阵生成\n",
    "def create_matrix_social(x,y,feature_fun,feature_type):  \n",
    "    origin_list = x.tolist()\n",
    "        \n",
    "    feature_list = list(map(feature_fun,origin_list))  \n",
    "    vectors = [vectorize(s,feature_type) for s in feature_list]\n",
    "    print('finished ....')\n",
    "    df = pd.DataFrame(vectors, columns=feature_type)\n",
    "    #df = pd.DataFrame(vectors)\n",
    "    \n",
    "    df['AGE'] = y.tolist()\n",
    "    #print(df['AGE'])\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "# from pandas.core.frame import DataFrame\n",
    "\n",
    "# import sys\n",
    "# maxInt = sys.maxsize\n",
    "# decrement = True\n",
    "# while decrement:\n",
    "#     decrement = False\n",
    "#     try:\n",
    "#         csv.field_size_limit(maxInt)\n",
    "#     except OverflowError:\n",
    "#         maxInt = int(maxInt/10)\n",
    "#         decrement = True\n",
    "\n",
    "\n",
    "# # def create_matrix_social(x,y,feature_fun,feature_type):  \n",
    "# data=x.to_frame()\n",
    "# loop = True\n",
    "# chunkSize = 1000\n",
    "# chunks = []\n",
    "# index=0\n",
    "# while loop:\n",
    "#     try:\n",
    "#         print(index)\n",
    "#         chunk = data.get_chunk(chunkSize)\n",
    "#         origin_list = chunk.tolist()    \n",
    "#         feature_list = list(map(feature_fun,origin_list))  \n",
    "#         vectors = [vectorize(s,feature_type) for s in feature_list]\n",
    "#         chunk2 = pd.DataFrame(vectors)\n",
    "\n",
    "#         chunks.append(chunk2)\n",
    "#         index+=1\n",
    "\n",
    "#     except StopIteration:\n",
    "#         loop = False\n",
    "#         print(\"Iteration is stopped.\")\n",
    "\n",
    "# print('开始合并')\n",
    "# df=pd.concat(chunks, ignore_index= True)\n",
    "# df.columns=feature_type\n",
    "# df['AGE'] = y.tolist()\n",
    "# #print(df['AGE'])\n",
    "# print(df.columns)\n",
    "# return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = pd.read_csv(\"pan13-author-profiling-training-corpus-2013-01-09/test1.csv\", sep=',',engine = 'python',iterator=True)\n",
    "# loop = True\n",
    "# chunkSize = 1000\n",
    "# chunks = []\n",
    "# index=0\n",
    "# while loop:\n",
    "#     try:\n",
    "#         print(index)\n",
    "#         chunk = data.get_chunk(chunkSize)\n",
    "#         origin_list = chunk    \n",
    "#         feature_list = list(map(lemma_feature,origin_list))  \n",
    "#         vectors = [vectorize(s,social_lemma_f) for s in feature_list]\n",
    "#         chunks.append(vectors)\n",
    "#         index+=1\n",
    "\n",
    "#     except StopIteration:\n",
    "#         loop = False\n",
    "#         print(\"Iteration is stopped.\")\n",
    "        \n",
    "# print('开始合并')\n",
    "# df=Dataframe(chunks)\n",
    "# #df = pd.concat(chunks, ignore_index= True)\n",
    "# df.columns=feature_type\n",
    "# df['AGE'] = social_train_y_new.tolist()\n",
    "# #print(df['AGE'])\n",
    "# print(df.columns)\n",
    "# df.to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lemma_test_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a51a70661ce4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msocial_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pan13-author-profiling-training-corpus-2013-01-09/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocial_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocial_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocial_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n"
     ]
    }
   ],
   "source": [
    "social_train = pd.read_csv(\"pan13-author-profiling-training-corpus-2013-01-09/train.csv\")\n",
    "print(social_train.head())\n",
    "print(len(social_train))\n",
    "print(social_train.columns)\n",
    "\n",
    "#social_train = social_train.drop([\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "\n",
    "print(type(social_train))\n",
    "social_train_x = social_train['text']\n",
    "social_train_y = social_train['age']\n",
    "print(len(social_train_x))\n",
    "print(type(social_train_x))\n",
    "print(len(social_train_y))\n",
    "print(type(social_train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  age\\r\\r\n",
      "0           0  Finding a Contractor;;If you decide that will ...        1\n",
      "1           1  You've got enable you to \"make a website\" 100 ...        3\n",
      "2           2  <img class=\"alignright\" src=\" ; alt=\"\" width=\"...        3\n",
      "3           3  These variety from the simplest issues such as...        3\n",
      "4           4  If you are heading to be in Dubai in transit o...        2\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv(\"pan13-author-profiling-training-corpus-2013-01-09/train1.csv\",lineterminator='\\n')\n",
    "test=pd.read_csv(\"pan13-author-profiling-training-corpus-2013-01-09/test1.csv\",lineterminator='\\n')\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<img class=\"alignright\" src=\" ; alt=\"\" width=\"208\" height=\"156\" />;Do you want to keep your home totally free from ;new york bed bugs;? In case you do, then you must know the following crucial factors. Bed bugs won\\'t be able to survive in regions where by there\\'s sufficient sunlight. Appropriately airing the interiors is usually a fantastic technique to cut down the possibilities of bed bugs infecting your house. Moreover, there are many other items that you simply ought to  sheets with your beds should be straightened out, the pillows need to be fluffed out. Bugs find crumpled apparel to become the ideal put for hiding and breeding. When you change and clean the sheets no less than once each week with very hot drinking water, it might help in protecting against the bugs from breeding. Drying the laundry inside a drier at greater temperature for for a longer time period can get rid of almost all of the bugs. ;;If you need to add further amount of defense against New York City bed bugs, you can insert the mattresses within a protector. This will help in creating a barrier between the mattress as well as your human body. On top of that, the bugs within the mattresses would not have the ability to flee out. In case you trap the bugs inside box springs, they\\'ll get suffocated. They wouldn’t have the opportunity to chunk the folks in your house and gradually they would die trapped  can also use steam cleansing being an useful approach for killing New York City bed bugs. This technique can flush out the bed bugs that cover deep inside the couches. You can set the maximum temperature, maintaining electrical wirings absent through the target space and then you can do many damage to the bed bugs  bugs may also enter your own home via various forms of media. In the event you do not know the place stuff has become, then you really should not convey it residence. deserted furniture are one of the principal areas which can be great breeding areas for bed bugs. They could also arrive hiding in electronics or tables. So, keep away from getting house anything at all from other people’s residence which has had been lying discarded for many time, specially something which was close towards the  you may have tried using each one of these techniques talked about above and have uncovered no success to get rid of bed bugs New York city from the home, you should search forward to calling expert pest control companies. ;Bed bug exterminator nyc;  expert services can assist you considerably in getting rid of these undesirable visitors.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_train_x = train['text']\n",
    "social_train_y = train['age\\r\\r']\n",
    "social_test_x = test['text']\n",
    "social_test_y = test['age\\r\\r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000    Do you really need new workwear to complete yo...\n",
      "2001    The Kyani Compensation Explained;;Kyani is a w...\n",
      "2002    &laquo; Fenugreek seeds which were soaked in w...\n",
      "2003    The biological results of nitric oxide are med...\n",
      "2004    Wife of one's whereabouts is a mystery I gave ...\n",
      "Name: text, dtype: object\n",
      "2000\n",
      "2000    3\n",
      "2001    3\n",
      "2002    3\n",
      "2003    2\n",
      "2004    2\n",
      "Name: age\\r\\r, dtype: int64\n",
      "finished ....\n",
      "Index(['wabasta', 'eighty-eighth', 'thrombocytopenia', '2threads', '200gb/s',\n",
      "       'rovio', 'consejos', 'home-owner', 'résumé', 'affordabilitythe',\n",
      "       ...\n",
      "       'fart', 'starcomms', 'funnily', 'plugging', 'workman', 'braking',\n",
      "       'bronze', 'sub-flooring', 'fittingly', 'AGE'],\n",
      "      dtype='object', length=137224)\n"
     ]
    }
   ],
   "source": [
    "social_train_x1=social_train_x[2000:4000]\n",
    "print(len(social_train_x1))\n",
    "print(social_train_x1.head())\n",
    "social_train_y1=social_train_y[2000:4000]\n",
    "print(len(social_train_y1))\n",
    "print(social_train_y1.head())\n",
    "create_matrix_social(social_train_x1,social_train_y1,lemma_feature,social_lemma_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lemma_train_matrix.csv\",header=None,mode='a')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  age\n",
      "0           0  The purpose in the Colorado serious estate is ...    3\n",
      "1           1  ; is Spanish language with regard to \"The M Wh...    3\n",
      "2           2  l am Ethan Qu Shanghai China, 24 years old.   ...    2\n",
      "3           3  I may not tell you this often but i want you t...    3\n",
      "4           4  Things were in the peak of affairs, a group of...    3\n",
      "17739\n",
      "Index(['Unnamed: 0', 'text', 'age'], dtype='object')\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "social_test = pd.read_csv(\"pan13-author-profiling-training-corpus-2013-01-09/test.csv\")\n",
    "print(social_test.head())\n",
    "print(len(social_test))\n",
    "print(social_test.columns)\n",
    "\n",
    "\n",
    "social_test_x = social_test['text']\n",
    "social_test_y = social_test['age']\n",
    "print(type(social_test_x))\n",
    "print(type(social_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished ....\n",
      "Index(['IN', 'CC', 'UH', 'VBD', ',', 'NNP', 'JJR', 'WP$', '$', 'VB', '``',\n",
      "       'LS', 'NN', ')', 'VBP', 'DT', 'JJ', 'SYM', 'CD', 'WDT', '''', 'NNS',\n",
      "       'MD', 'RBS', 'VBN', 'RB', 'JJS', 'WRB', 'PDT', '#', 'RBR', ':', '.',\n",
      "       'WP', 'RP', 'EX', '(', 'PRP$', 'NNPS', 'PRP', 'POS', 'VBZ', 'TO', 'VBG',\n",
      "       'FW', 'AGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "create_matrix_social(social_train_x,social_train_y,pos_feature,social_pos_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_trian_pos_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finding a Contractor;;If you decide that will ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've got enable you to \"make a website\" 100 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;img class=\"alignright\" src=\" ; alt=\"\" width=\"...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These variety from the simplest issues such as...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you are heading to be in Dubai in transit o...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  age\n",
       "0  Finding a Contractor;;If you decide that will ...    1\n",
       "1  You've got enable you to \"make a website\" 100 ...    3\n",
       "2  <img class=\"alignright\" src=\" ; alt=\"\" width=\"...    3\n",
       "3  These variety from the simplest issues such as...    3\n",
       "4  If you are heading to be in Dubai in transit o...    2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(social_train_x)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished ....\n",
      "Index([')', '#', ',', 'FW', 'IN', 'JJS', 'RBS', 'PRP$', 'VBN', '``', 'VBZ',\n",
      "       'JJ', 'WRB', 'RP', '$', 'SYM', 'VBG', 'RBR', 'NNP', '.', 'RB', 'NN',\n",
      "       'CD', 'CC', ':', 'PRP', '''', 'NNS', 'WP', 'VB', 'VBD', '(', 'UH',\n",
      "       'VBP', 'POS', 'JJR', 'WP$', 'PDT', 'EX', 'DT', 'TO', 'LS', 'NNPS', 'MD',\n",
      "       'WDT', 'AGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#create_matrix_social(social_train_x,social_train_y,pos_feature,social_pos_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_pos_train_matrix.csv\")\n",
    "create_matrix_social(social_test_x,social_test_y,pos_feature,social_pos_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_pos_test_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(social_train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17743\n",
      "[]\n",
      "5912\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#查找不良数据  trian和test里面\n",
    "def find_error(x):  \n",
    "    origin_list = np.array(x).tolist()\n",
    "    print(len(origin_list))\n",
    "    error_list = []\n",
    "    flag = 0\n",
    "    for i in origin_list:\n",
    "        flag=flag+1\n",
    "        #if flag==872:\n",
    "        if pd.isnull(i):    \n",
    "            print(flag-1)\n",
    "            error_list.append(flag-1)\n",
    "            continue\n",
    "        #print('this is '+str(flag)+\"th instance....\")\n",
    "    print(error_list)\n",
    "    \n",
    "find_error(social_train_x)\n",
    "find_error(social_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17745\n",
      "Index(['text'], dtype='object')\n",
      "17743\n",
      "17745\n",
      "17743\n",
      "5915\n",
      "5912\n",
      "5915\n",
      "5912\n",
      "Index(['text', 'age\\r'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(17743, 2)\n",
      "Index(['text', 'age\\r'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(5912, 2)\n"
     ]
    }
   ],
   "source": [
    "#去掉train 和test中的不良数据块\n",
    "train_error_list = [1473, 3216]\n",
    "test_error_list = [1212, 1429, 2610]\n",
    "print(len(social_train_x))\n",
    "print(social_train_x.to_frame().columns)\n",
    "social_train_x_new = social_train_x.drop(train_error_list,inplace=False)\n",
    "print(len(social_train_x_new))\n",
    "\n",
    "print(len(social_train_y))\n",
    "social_train_y_new = social_train_y.drop(train_error_list,inplace=False)\n",
    "print(len(social_train_y_new))\n",
    "\n",
    "print(len(social_test_x))\n",
    "social_test_x_new = social_test_x.drop(test_error_list,inplace=False)\n",
    "print(len(social_test_x_new))\n",
    "\n",
    "print(len(social_test_y))\n",
    "social_test_y_new = social_test_y.drop(test_error_list,inplace=False)\n",
    "print(len(social_test_y_new))\n",
    "\n",
    "train1=pd.concat([social_train_x_new,social_train_y_new,],axis=1)\n",
    "print(train1.columns)\n",
    "print(type(train1))\n",
    "print(train1.shape)\n",
    "train1.to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/train1.csv\")\n",
    "\n",
    "test1=pd.concat([social_test_x_new,social_test_y_new,],axis=1)\n",
    "print(test1.columns)\n",
    "print(type(test1))\n",
    "print(test1.shape)\n",
    "test1.to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-a8ef3e1747df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#生成social集的矩阵\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#create_matrix_social(social_train_x,social_train_y,lemma_feature,social_lemma_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lemma_train_matrix.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcreate_matrix_social\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocial_test_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msocial_test_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlemma_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msocial_lemma_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pan13-author-profiling-training-corpus-2013-01-09/social_lemma_test_matrix.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# #create_matrix_social(social_train_x_new,social_train_y_new,pos_feature,social_pos_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_pos_train_matrix.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# #create_matrix_social(social_test_x_new,social_test_y_new,pos_feature,social_pos_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_pos_test_matrix.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-bd4281df3bed>\u001b[0m in \u001b[0;36mcreate_matrix_social\u001b[1;34m(x, y, feature_fun, feature_type)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfeature_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_fun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morigin_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finished ....'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-bd4281df3bed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfeature_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_fun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morigin_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finished ....'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-903429b121b7>\u001b[0m in \u001b[0;36mvectorize\u001b[1;34m(sent, feature_list)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train和test set 的x分别向量化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(vectorize(['LI', 'hurrying', 'beckoning', 'beneficent', 'Scholar', 'rarely', 'faltering'],ab_lemma_f))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-903429b121b7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train和test set 的x分别向量化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(vectorize(['LI', 'hurrying', 'beckoning', 'beneficent', 'Scholar', 'rarely', 'faltering'],ab_lemma_f))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#生成all_author集的矩阵\n",
    "# create_matrix_all(all_train_x,all_train_y,lemma_feature,all_lemma_f).to_csv(\"all_lemma_train_matrix.csv\")\n",
    "# create_matrix_all(all_test_x,all_test_y,lemma_feature,all_lemma_f).to_csv(\"all_lemma_test_matrix.csv\")\n",
    "# create_matrix_all(all_train_x,all_train_y,pos_feature,all_pos_f).to_csv(\"all_pos_train_matrix.csv\")\n",
    "# create_matrix_all(all_test_x,all_test_y,pos_feature,all_pos_f).to_csv(\"all_pos_test_matrix.csv\")\n",
    "# create_matrix_all(all_train_x,all_train_y,lexical_feature,all_lexical_f).to_csv(\"all_lexical_train_matrix.csv\")\n",
    "# create_matrix_all(all_test_x,all_test_y,lexical_feature,all_lexical_f).to_csv(\"all_lexcial_test_matrix.csv\")\n",
    "#生成social集的矩阵\n",
    "#create_matrix_social(social_train_x,social_train_y,lemma_feature,social_lemma_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lemma_train_matrix.csv\")\n",
    "create_matrix_social(social_test_x,social_test_y,lemma_feature,social_lemma_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lemma_test_matrix.csv\")\n",
    "# #create_matrix_social(social_train_x_new,social_train_y_new,pos_feature,social_pos_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_pos_train_matrix.csv\")\n",
    "# #create_matrix_social(social_test_x_new,social_test_y_new,pos_feature,social_pos_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_pos_test_matrix.csv\")\n",
    "# create_matrix_social(social_train_x_new,social_train_y_new,lexical_feature,social_lexical_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lexical_train_matrix.csv\")\n",
    "# create_matrix_social(social_test_x_new,social_test_y_new,lexical_feature,social_lexical_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lexcial_test_matrix.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "0    Finding a Contractor;;If you decide that will ...\n",
      "1    You've got enable you to \"make a website\" 100 ...\n",
      "2    <img class=\"alignright\" src=\" ; alt=\"\" width=\"...\n",
      "3    These variety from the simplest issues such as...\n",
      "4    If you are heading to be in Dubai in transit o...\n",
      "Name: text, dtype: object\n",
      "2500\n",
      "0    1\n",
      "1    3\n",
      "2    3\n",
      "3    3\n",
      "4    2\n",
      "Name: age\\r\\r, dtype: int64\n",
      "finished ....\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-abe0f7790c19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocial_train_y2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocial_train_y2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcreate_matrix_social\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocial_train_x2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msocial_train_y2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlexical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msocial_lexical_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pan13-author-profiling-training-corpus-2013-01-09/social_lexical_matrix1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-bd4281df3bed>\u001b[0m in \u001b[0;36mcreate_matrix_social\u001b[1;34m(x, y, feature_fun, feature_type)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finished ....'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m#df = pd.DataFrame(vectors)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m   7473\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7474\u001b[0m         return _list_to_arrays(data, columns, coerce_float=coerce_float,\n\u001b[1;32m-> 7475\u001b[1;33m                                dtype=dtype)\n\u001b[0m\u001b[0;32m   7476\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7477\u001b[0m         return _list_of_dict_to_arrays(data, columns,\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m   7550\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7551\u001b[0m         \u001b[1;31m# list of lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7552\u001b[1;33m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_object_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7553\u001b[0m     return _convert_object_array(content, columns, dtype=dtype,\n\u001b[0;32m   7554\u001b[0m                                  coerce_float=coerce_float)\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.to_object_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "social_train_x2=social_train_x[0:2500]\n",
    "print(len(social_train_x2))\n",
    "print(social_train_x2.head())\n",
    "social_train_y2=social_train_y[0:2500]\n",
    "print(len(social_train_y2))\n",
    "print(social_train_y2.head())   \n",
    "create_matrix_social(social_train_x2,social_train_y2,lexical_feature,social_lexical_f).to_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lexical_matrix1.csv\",mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-284d31dbf6f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pan13-author-profiling-training-corpus-2013-01-09/social_lemma_train_matrix.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"pan13-author-profiling-training-corpus-2013-01-09/social_lemma_train_matrix.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if sys.path[0] == '':\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR.intercept:\n",
      " 1905.545978972686\n",
      "LR.coef:\n",
      " [-1.04630299  0.02965821  0.27095086 ...  0.00249867  0.00489886\n",
      "  0.00267346]\n",
      "R^2:\n",
      " -0.04096779878014356\n",
      "MSE: 91.83649247015931\n",
      "RMSE: 9.583135836987772\n",
      "y_predict:\n",
      " [1904.71428971 1906.90184725 1903.42905493]\n",
      "y_test:\n",
      " 0    1910\n",
      "1    1922\n",
      "2    1899\n",
      "Name: YEAR, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sklearn 线性回归\n",
    "import numpy as np\n",
    "from sklearn import linear_model,datasets,metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.read_csv(\"DCLSA/la_dataset_split/ab/ab_lemma_train_matrix.csv\")\n",
    "train_data.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "test_data = pd.read_csv(\"DCLSA/la_dataset_split/ab/ab_lemma_test_matrix.csv\")\n",
    "test_data.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "\n",
    "X_train = train_data.iloc[:,0:-1]\n",
    "y_train = train_data.ix[:,-1]\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "X_test = test_data.iloc[:,0:-1]\n",
    "y_test = test_data.ix[:,-1]\n",
    "\n",
    "\n",
    "from sklearn.linear_model import  LinearRegression\n",
    "LR = LinearRegression()\n",
    "### 对训练数据进行拟合训练\n",
    "LR.fit(X_train, y_train)\n",
    "### 输出参数,分别是截距（intercept_）和权重参数(coef_）\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
    "# ridge_reg.fit(X, y)\n",
    "# ridge_reg.predict([[1.5]])\n",
    " \n",
    " \n",
    "# ##执行SGD的岭回归\n",
    "# sgd_reg=SGDRegressor(penalty=\"l2\")\n",
    "# #sgd_reg.fit(X,y#这样也行，但有警告‘y = column_or_1d(y, warn=True\n",
    "# sgd_reg.fit(X,y.ravel())\n",
    "# sgd_reg.predict([[1.5]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('LR.intercept:\\n',LR.intercept_)\n",
    "print('LR.coef:\\n',LR.coef_)\n",
    "### 计算确定系数R^2,取值范[0,1],值越大,说明模拟的拟合度越好，对模型的解释能力越强\n",
    "print('R^2:\\n',LR.score(X_test,y_test))\n",
    "### 根据测试数据计算预测值y_predict\n",
    "y_predict=LR.predict(X_test)\n",
    "### MSE为均方误差，用测试数据来验证，MSE为预测数据和测试数据误差平方和的均值\n",
    "print (\"MSE:\",metrics.mean_squared_error(y_test,y_predict))\n",
    "### RMSE为均方根误差\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_predict)))\n",
    "\n",
    "\n",
    "print('y_predict:\\n',y_predict[0:5])    ### 输出预测值前5行\n",
    "print('y_test:\\n',y_test[0:5])          ### 输出测试值前5行\n",
    "plt.scatter(y_test,y_predict,c='b',alpha=0.5,marker='*')   \n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_predict')\n",
    "plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'k--',lw=4)   ### 画出y=x这条线\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'python', '.']\n",
      "student\n"
     ]
    }
   ],
   "source": [
    "l = nltk.word_tokenize(\"In the python.\")\n",
    "print(l)\n",
    "p = PorterStemmer()\n",
    "stem = p.stem(\"Students\")\n",
    "print(stem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
